<section>
    <section>
        <h3>Evolving Learning Methods
    </section>
    <section>
        <h6>Neuroevolution</h6>
        <ul>
            <small>
            <li><b>Direct evolution of neural network weights</b><br/>
                Evolutionary Strategies</li>
            <li><b>Direct evolution of neural network weights and structure</b><br/>
                NEAT, CGPANN</li>
            <li><b>Indirect evolution of neural network weights</b><br/>
                HyperNEAT, Deep Neuroevolution</li>
            <li><b>Evolution of neural network structure</b><br/>
                Deep Neuroevolution, Evolutionary Neural Architecture Search</li>
            <li><b>Evolution of learning in neural networks</b><br/>
                Cellular Encoding, Developmental Neural Networks, Deep Neuromodulation</li>
            </small>
        </ul>
    </section>
    <section>
        <h6>Stochastic Gradient Descent</h6>
        <img data-src="video/sgd.gif" width="90%" height="auto">
        <br />
        SGD(<span style="color: #FF7F00">$\eta$</span>, <span style="color:
        #6A3D9A">$\alpha$</span>)
    </section>
    <section>
        <h6>Adaptive Moment Estimation (Adam)</h6>
        <img data-src="video/adam.gif" width="90%" height="auto">
        <br />
        Adam(<span style="color: #FF7F00">$\eta$</span>, <span style="color:
        #6A3D9A">$\beta_1$</span>, <span style="color: #6A3D9A">$\beta_2$</span>,
        <span style="color: #33A02C">$\epsilon$</span>)
    </section>
    <section>
        <h6>Learning Parameters</h6>
        <table>
            <tr>
                <td>Inputs</td>
                <td>Output (SGD) &nbsp; &nbsp;</td>
                <td>Output (Adam)</td>
            </tr>
            <tr>
                <td>$\mu_{\theta}$</td>
                <td>$\eta$</td>
                <td>$\eta$</td>
            </tr>
            <tr>
                <td>$\sigma_{\theta}$</td>
                <td>$\alpha$</td>
                <td>$\beta_1$</td>
            </tr>
            <tr>
                <td>$\mu_{\nabla Q}$</td>
                <td></td>
                <td>$\beta_2$</td>
            </tr>
            <tr>
                <td>$\sigma_{\nabla Q}$</td>
                <td></td>
                <td>$\epsilon$</td>
            </tr>
            <tr>
                <td>layer location</td>
                <td></td>
                <td></td>
            </tr>
            <tr>
                <td>layer size</td>
                <td></td>
                <td></td>
            </tr>
        </table>
    </section>
    <section>
        <h6>Neuromodulation</h6>
        <img data-src="img/neuromod/neuromod_scheme.png" width="80%" height="auto">
        <p class="source">
            Wilson, Dennis G., et al. "Neuromodulated Learning in Deep Neural Networks."<br/>arXiv preprint arXiv:1812.03365 (2018).
        </p>
    </section>
    <section>
        <h6>Image classification: CIFAR-10</h6>
        <img data-src="img/neuromod/cifar10_train-page1.png" width="80%" height="auto">
        <br />
        <p class="fragment">
        <img data-src="img/neuromod/cifar10_test-page1.png" width="80%" height="auto">
        </p>
    </section>
    <section>
        <h6>Image classification: CIFAR-100</h6>
        <img data-src="img/neuromod/cifar100_train-page1.png" width="80%" height="auto">
        <br />
        <p class="fragment">
        <img data-src="img/neuromod/cifar100_test-page1.png" width="80%" height="auto">
        </p>
    </section>
    <section>
        <h6>AGRN Outputs</h6>
        <img data-src="img/neuromod/outputs_Nm-Adam_m1_lr-page1.png" width="40%" height="auto">
        <img data-src="img/neuromod/outputs_Nm-Adam_m1_beta1-page1.png" width="40%" height="auto">
        <br />
        <img data-src="img/neuromod/outputs_Nm-Adam_m1_beta2-page1.png" width="40%" height="auto">
        <img data-src="img/neuromod/outputs_Nm-Adam_m1_epsilon-page1.png" width="40%" height="auto">
    </section>
    <section>
        Evolution finds local and dynamic neuromodulation strategies which
        improve on existing learning methods.
    </section>
</section>
